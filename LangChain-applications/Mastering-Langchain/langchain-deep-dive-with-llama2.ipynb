{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -qqq langchain accelerate bitsandbytes\n!pip install -qqq transformers==4.33.2\n!pip install -qqq optimum==1.13.1\n!pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/ ","metadata":{"execution":{"iopub.status.busy":"2023-12-02T06:53:10.677328Z","iopub.execute_input":"2023-12-02T06:53:10.677705Z","iopub.status.idle":"2023-12-02T06:54:00.671779Z","shell.execute_reply.started":"2023-12-02T06:53:10.677669Z","shell.execute_reply":"2023-12-02T06:54:00.670452Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Looking in indexes: https://pypi.org/simple, https://huggingface.github.io/autogptq-index/whl/cu118/\nCollecting auto-gptq\n  Downloading https://huggingface.github.io/autogptq-index/whl/cu118/auto-gptq/auto_gptq-0.5.1%2Bcu118-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: accelerate>=0.22.0 in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (0.24.1)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (2.1.0)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (0.1.99)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (1.24.3)\nCollecting rouge (from auto-gptq)\n  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\nCollecting gekko (from auto-gptq)\n  Downloading gekko-1.0.6-py3-none-any.whl (12.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (2.0.0)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (0.4.0)\nRequirement already satisfied: transformers>=4.31.0 in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (4.33.2)\nCollecting peft>=0.5.0 (from auto-gptq)\n  Obtaining dependency information for peft>=0.5.0 from https://files.pythonhosted.org/packages/14/0b/8402305043884c76a9d98e5e924c3f2211c75b02acd5b742e6c45d70506d/peft-0.6.2-py3-none-any.whl.metadata\n  Downloading peft-0.6.2-py3-none-any.whl.metadata (23 kB)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (4.66.1)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.22.0->auto-gptq) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.22.0->auto-gptq) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.22.0->auto-gptq) (6.0.1)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.22.0->auto-gptq) (0.17.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->auto-gptq) (2023.8.8)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->auto-gptq) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->auto-gptq) (0.13.3)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq) (11.0.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq) (2.0.3)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq) (0.70.15)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq) (2023.10.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq) (3.8.5)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq) (0.18.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from rouge->auto-gptq) (1.16.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (3.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (1.3.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate>=0.22.0->auto-gptq) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->auto-gptq) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->auto-gptq) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->auto-gptq) (2023.7.22)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->auto-gptq) (2.1.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->auto-gptq) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->auto-gptq) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->auto-gptq) (2023.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->auto-gptq) (1.3.0)\nDownloading peft-0.6.2-py3-none-any.whl (174 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.7/174.7 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: rouge, gekko, peft, auto-gptq\nSuccessfully installed auto-gptq-0.5.1+cu118 gekko-1.0.6 peft-0.6.2 rouge-1.0.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\nimport accelerate\nfrom langchain import HuggingFacePipeline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2023-12-02T07:05:28.025196Z","iopub.execute_input":"2023-12-02T07:05:28.026252Z","iopub.status.idle":"2023-12-02T07:05:28.463105Z","shell.execute_reply.started":"2023-12-02T07:05:28.026203Z","shell.execute_reply":"2023-12-02T07:05:28.462084Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# LLMs","metadata":{}},{"cell_type":"code","source":"model_name = \"TheBloke/Llama-2-7b-Chat-GPTQ\"\n\nmodel = AutoModelForCausalLM.from_pretrained(model_name,\n                                             device_map=\"auto\",\n                                             trust_remote_code=True,\n                                             revision=\"main\")\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-02T06:59:10.066969Z","iopub.execute_input":"2023-12-02T06:59:10.067909Z","iopub.status.idle":"2023-12-02T07:00:00.736428Z","shell.execute_reply.started":"2023-12-02T06:59:10.067863Z","shell.execute_reply":"2023-12-02T07:00:00.735405Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading config.json:   0%|          | 0.00/789 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7b7b254b0c34cc6b18fdd60ce680654"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/3.90G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"beb2905636e946ef819e30fc3c04d745"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a4c1d5216fb489bb64df37b7da0d294"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer_config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e2cf57c527d4231a6261e7f10e68c47"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"742a2282d4ac4289bb666e47f75b5797"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0902863105204186b5641b1548442db7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac423a04ec0e42c598be501ddea8a854"}},"metadata":{}}]},{"cell_type":"code","source":"text_pipeline = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    return_full_text=True,\n    max_new_tokens = 1024,\n    top_p = 0.95,\n    do_sample = True,\n    repetition_penalty = 1.1,\n)\n\nllm = HuggingFacePipeline(pipeline=text_pipeline, model_kwargs={\"temperature\": 0})","metadata":{"execution":{"iopub.status.busy":"2023-12-02T07:12:30.477212Z","iopub.execute_input":"2023-12-02T07:12:30.477565Z","iopub.status.idle":"2023-12-02T07:12:30.483348Z","shell.execute_reply.started":"2023-12-02T07:12:30.477537Z","shell.execute_reply":"2023-12-02T07:12:30.482350Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"%%time\nresult = llm(\"Explain the difference between Vector Database and Vector Indexing libraries in a couple of lines.\")","metadata":{"execution":{"iopub.status.busy":"2023-12-02T07:13:26.624472Z","iopub.execute_input":"2023-12-02T07:13:26.624942Z","iopub.status.idle":"2023-12-02T07:14:24.036463Z","shell.execute_reply.started":"2023-12-02T07:13:26.624902Z","shell.execute_reply":"2023-12-02T07:14:24.035504Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"CPU times: user 40.8 s, sys: 16.6 s, total: 57.4 s\nWall time: 57.4 s\n","output_type":"stream"}]},{"cell_type":"code","source":"print(result)","metadata":{"execution":{"iopub.status.busy":"2023-12-02T07:14:24.038080Z","iopub.execute_input":"2023-12-02T07:14:24.038362Z","iopub.status.idle":"2023-12-02T07:14:24.043327Z","shell.execute_reply.started":"2023-12-02T07:14:24.038335Z","shell.execute_reply":"2023-12-02T07:14:24.042260Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"\nVector databases and vector indexing libraries are both used to optimize queries on large-scale datasets, but they serve different purposes. Vector databases store pre-calculated vector representations of data, allowing for efficient querying and clustering. Vector indexing libraries, on the other hand, create and update vector representations of data incrementally, enabling fast similarity searching and other query types.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Chains","metadata":{}},{"cell_type":"code","source":"%%time\nfrom langchain.chains import ConversationChain\n\nchain = ConversationChain(\n    llm=llm,\n    verbose=True\n)\n\nchain.run('Explain the difference between Vector Database and Vector Indexing libraries in a couple of lines.')","metadata":{"execution":{"iopub.status.busy":"2023-12-02T07:20:16.575240Z","iopub.execute_input":"2023-12-02T07:20:16.575605Z","iopub.status.idle":"2023-12-02T07:22:59.609538Z","shell.execute_reply.started":"2023-12-02T07:20:16.575575Z","shell.execute_reply":"2023-12-02T07:22:59.608639Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"\n\n\u001b[1m> Entering new ConversationChain chain...\u001b[0m\nPrompt after formatting:\n\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n\nCurrent conversation:\n\nHuman: Explain the difference between Vector Database and Vector Indexing libraries in a couple of lines.\nAI:\u001b[0m\n\n\u001b[1m> Finished chain.\u001b[0m\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"\" Of course! 😊 Vector Database and Vector Indexing are two popular NLP libraries used for efficient text processing. The main distinction is that Vector Database stores vectors directly, while Vector Indexing generates them on demand. 🔥 To elaborate, Vector Database is essentially a fixed-size vector space where each document is represented by a vector in that space. On the other hand, Vector Indexing creates dense vector representations of documents by applying a transformation to their bag-of-words (BOW) representation. 💡 These transformed vectors are then stored in a sparse matrix, which can be queried using an inverted index. So, while Vector Database requires more memory upfront, it offers faster lookup times since the entire vector space is precomputed. Conversely, Vector Indexing requires less memory but needs to perform computations during query time. Now, if you want a more detailed explanation or have follow-up questions, feel free to ask! I'm here to help! 🙌\""},"metadata":{}}]},{"cell_type":"code","source":"chain.run('What was my previous question?')","metadata":{"execution":{"iopub.status.busy":"2023-12-02T07:23:13.153087Z","iopub.execute_input":"2023-12-02T07:23:13.153440Z","iopub.status.idle":"2023-12-02T07:23:32.316955Z","shell.execute_reply.started":"2023-12-02T07:23:13.153414Z","shell.execute_reply":"2023-12-02T07:23:32.316001Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"\n\n\u001b[1m> Entering new ConversationChain chain...\u001b[0m\nPrompt after formatting:\n\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n\nCurrent conversation:\nHuman: Explain the difference between Vector Database and Vector Indexing libraries in a couple of lines.\nAI:  Of course! 😊 Vector Database and Vector Indexing are two popular NLP libraries used for efficient text processing. The main distinction is that Vector Database stores vectors directly, while Vector Indexing generates them on demand. 🔥 To elaborate, Vector Database is essentially a fixed-size vector space where each document is represented by a vector in that space. On the other hand, Vector Indexing creates dense vector representations of documents by applying a transformation to their bag-of-words (BOW) representation. 💡 These transformed vectors are then stored in a sparse matrix, which can be queried using an inverted index. So, while Vector Database requires more memory upfront, it offers faster lookup times since the entire vector space is precomputed. Conversely, Vector Indexing requires less memory but needs to perform computations during query time. Now, if you want a more detailed explanation or have follow-up questions, feel free to ask! I'm here to help! 🙌\nHuman: What was my previous question?\nAI:\u001b[0m\n\n\u001b[1m> Finished chain.\u001b[0m\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"' Your previous question was: \"Explain the difference between Vector Database and Vector Indexing libraries in a couple of lines.\"'"},"metadata":{}}]},{"cell_type":"markdown","source":"#### So you can see that by providing some memory to the LM, we were able to augment it and have a more interesting conversation with it!","metadata":{}},{"cell_type":"markdown","source":"# Prompt Templates","metadata":{}},{"cell_type":"code","source":"from langchain.prompts import PromptTemplate\n\ntemplate = \"\"\"\nReturn all the subcategories of the following category\n\n{category}\n\"\"\"\n\nprompt = PromptTemplate(\n    input_variables=['category'],\n    template=template\n)\n\nprompt","metadata":{"execution":{"iopub.status.busy":"2023-12-02T07:25:18.288461Z","iopub.execute_input":"2023-12-02T07:25:18.289333Z","iopub.status.idle":"2023-12-02T07:25:18.306503Z","shell.execute_reply.started":"2023-12-02T07:25:18.289298Z","shell.execute_reply":"2023-12-02T07:25:18.305631Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"PromptTemplate(input_variables=['category'], template='\\nReturn all the subcategories of the following category\\n\\n{category}\\n')"},"metadata":{}}]},{"cell_type":"code","source":"from langchain.chains import LLMChain\n\nchain = LLMChain(\n    llm=llm,\n    prompt=prompt,\n    verbose=True\n)\n\nchain.run('Vector Database')","metadata":{"execution":{"iopub.status.busy":"2023-12-02T07:26:10.246507Z","iopub.execute_input":"2023-12-02T07:26:10.246851Z","iopub.status.idle":"2023-12-02T07:27:04.214656Z","shell.execute_reply.started":"2023-12-02T07:26:10.246811Z","shell.execute_reply":"2023-12-02T07:27:04.213799Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"\n\n\u001b[1m> Entering new LLMChain chain...\u001b[0m\nPrompt after formatting:\n\u001b[32;1m\u001b[1;3m\nReturn all the subcategories of the following category\n\nVector Database\n\u001b[0m\n\n\u001b[1m> Finished chain.\u001b[0m\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"'\\nThe following are the subcategories under Vector Database:\\n\\n1. Linear Algebra\\n2. Mathematical Optimization\\n3. Statistical Computing\\n4. Computer Vision\\n5. Robotics\\n6. Data Science\\n7. Machine Learning\\n8. Numerical Computation\\n9. Scientific Computing'"},"metadata":{}}]},{"cell_type":"code","source":"template = \"\"\"\n\n[INST] <<SYS>>\nAct as a Senior Machine Learning engineer who is aware of all the recent developments in the Artificial Intelligence space.\n<</SYS>>\n\n{text}[/INST]\n\"\"\"\n\nprompt = PromptTemplate(\n    input_variables=[\"text\"],\n    template=template,\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-02T07:48:44.006635Z","iopub.execute_input":"2023-12-02T07:48:44.007026Z","iopub.status.idle":"2023-12-02T07:48:44.011981Z","shell.execute_reply.started":"2023-12-02T07:48:44.006994Z","shell.execute_reply":"2023-12-02T07:48:44.011103Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"text = \"What is a Retrieval-augmented generation ?\"\nprint(prompt.format(text=text))","metadata":{"execution":{"iopub.status.busy":"2023-12-02T07:58:17.181280Z","iopub.execute_input":"2023-12-02T07:58:17.182044Z","iopub.status.idle":"2023-12-02T07:58:17.188427Z","shell.execute_reply.started":"2023-12-02T07:58:17.182011Z","shell.execute_reply":"2023-12-02T07:58:17.187174Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"\n\n[INST] <<SYS>>\nAct as a Senior Machine Learning engineer who is aware of all the recent developments in the Artificial Intelligence space.\n<</SYS>>\n\nWhat is a Retrieval-augmented generation ?[/INST]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"chain = LLMChain(\n    llm=llm,\n    prompt=prompt,\n    verbose=True\n)\n\nchain.run(text)","metadata":{"execution":{"iopub.status.busy":"2023-12-02T07:58:19.667859Z","iopub.execute_input":"2023-12-02T07:58:19.668676Z","iopub.status.idle":"2023-12-02T08:05:47.426844Z","shell.execute_reply.started":"2023-12-02T07:58:19.668644Z","shell.execute_reply":"2023-12-02T08:05:47.425865Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"\n\n\u001b[1m> Entering new LLMChain chain...\u001b[0m\nPrompt after formatting:\n\u001b[32;1m\u001b[1;3m\n\n[INST] <<SYS>>\nAct as a Senior Machine Learning engineer who is aware of all the recent developments in the Artificial Intelligence space.\n<</SYS>>\n\nWhat is a Retrieval-augmented generation ?[/INST]\n\u001b[0m\n\n\u001b[1m> Finished chain.\u001b[0m\n","output_type":"stream"},{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"'Ah, an excellent question! As a Senior Machine Learning engineer, I can tell you that Retrieval-augmented generation (RAG) is a cutting-edge approach in natural language processing (NLP) that has gained significant attention in recent years. It\\'s a technique that combines the strengths of both generative and retrieval-based models to create more accurate and diverse language models.\\nIn traditional language modeling, researchers use generative models like transformer-based autoencoders or variational autoencoders (VAEs) to learn the distribution of words in a language. These models are trained on large datasets of text and have been instrumental in advancing the state of the art in NLP. However, they have limitations when it comes to generating coherent and relevant text.\\nRetrieval-augmented generation addresses these limitations by combining the strengths of generative and retrieval-based models. The basic idea is to use a retrieval-based model to search for relevant passages from a large corpus of text, and then use these passages as \"building blocks\" to generate new text. This not only improves the quality and relevance of the generated text but also enables the model to handle long-tail distributions and minimize the risk of exposure bias.\\nThere are several ways to implement RAG, including:\\n1. Memory-augmented language models: These models use a memory component to store previously encountered words or phrases, which can be retrieved and reused during generation. This allows the model to generate text that is more coherent and contextually appropriate.\\n2. Generative retrieval: In this approach, the generator network is trained to produce a sequence of words that are likely to occur together in the target language. The discriminator network is trained to distinguish between real and fake sequences, and the generator network is trained to maximize the likelihood of producing realistic sequences.\\n3. Multi-task learning: In this approach, the same model is used for both generation and retrieval tasks. The generator network learns to generate text while the discriminator network learns to distinguish between real and fake sequences.\\nThe advantages of RAG are numerous, including improved ability to generate coherent and relevant text, better handling of long-tail distributions, and reduced risk of exposure bias. It has shown promising results in various NLP tasks such as text completion, language translation, and dialogue systems.\\nHowever, it\\'s important to note that RAG is still an emerging area of research, and there are many open challenges and questions related to its application and interpretation. As a Senior Machine Learning engineer, I would advise my clients to stay tuned for further advancements in this field, as it has the potential to revolutionize the way we interact with language.'"},"metadata":{}}]},{"cell_type":"markdown","source":"When you build conversational applications, it's often interesting to be able to break down the prompts into sub elements.\nLet me show you how to do this.","metadata":{}},{"cell_type":"code","source":"chain","metadata":{"execution":{"iopub.status.busy":"2023-12-02T08:35:22.476562Z","iopub.execute_input":"2023-12-02T08:35:22.476962Z","iopub.status.idle":"2023-12-02T08:35:22.484408Z","shell.execute_reply.started":"2023-12-02T08:35:22.476928Z","shell.execute_reply":"2023-12-02T08:35:22.483350Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"LLMChain(verbose=True, prompt=PromptTemplate(input_variables=['text'], template='\\n\\n[INST] <<SYS>>\\nAct as a Senior Machine Learning engineer who is aware of all the recent developments in the Artificial Intelligence space.\\n<</SYS>>\\n\\n{text}[/INST]\\n'), llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x79cc2c3e2e60>, model_kwargs={'temperature': 0}))"},"metadata":{}}]},{"cell_type":"code","source":"from langchain.prompts import (\n    SystemMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n    ChatPromptTemplate\n)\n\nfrom langchain.schema import AIMessage, HumanMessage\n\nsystem_template = \"\"\"\nAct as a Senior Machine Learning engineer who is aware of all the recent developments in the Artificial Intelligence space.\nAlways give real world examples with analogies\"\n\"\"\"\n\nhuman_template = '{text}'\n\nsystem_message = SystemMessagePromptTemplate.from_template(\n    system_template\n)\n\nhuman_message = HumanMessagePromptTemplate.from_template(\n    human_template\n)\n\nAI_message = AIMessage(content=\"Welcome to Chatbot!\")\n\nprint(f\"system_message: {system_message}\\n\")\nprint(f\"human_message: {human_message}\\n\")\nprint(f\"AI_message: {AI_message}\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-12-02T08:50:16.134950Z","iopub.execute_input":"2023-12-02T08:50:16.136165Z","iopub.status.idle":"2023-12-02T08:50:16.143812Z","shell.execute_reply.started":"2023-12-02T08:50:16.136117Z","shell.execute_reply":"2023-12-02T08:50:16.142899Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"system_message: prompt=PromptTemplate(input_variables=[], template='\\nAct as a Senior Machine Learning engineer who is aware of all the recent developments in the Artificial Intelligence space.\\nAlways give real world examples with analogies\"\\n')\n\nhuman_message: prompt=PromptTemplate(input_variables=['text'], template='{text}')\n\nAI_message: content='Welcome to Chatbot!'\n\n","output_type":"stream"}]},{"cell_type":"code","source":"prompt = ChatPromptTemplate.from_messages([\n    system_message,\n    human_message,\n    AI_message\n])\n\nchain = LLMChain(\n    llm=llm,\n    prompt=prompt,\n    verbose=True\n)\n\nchain.run(\"What are self-attention layers in a Transformer?\")","metadata":{"execution":{"iopub.status.busy":"2023-12-02T08:50:59.248175Z","iopub.execute_input":"2023-12-02T08:50:59.248980Z","iopub.status.idle":"2023-12-02T08:52:47.670987Z","shell.execute_reply.started":"2023-12-02T08:50:59.248946Z","shell.execute_reply":"2023-12-02T08:52:47.670058Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"\n\n\u001b[1m> Entering new LLMChain chain...\u001b[0m\nPrompt after formatting:\n\u001b[32;1m\u001b[1;3mSystem: \nAct as a Senior Machine Learning engineer who is aware of all the recent developments in the Artificial Intelligence space.\nAlways give real world examples with analogies\"\n\nHuman: What are self-attention layers in a Transformer?\nAI: Welcome to Chatbot!\u001b[0m\n\n\u001b[1m> Finished chain.\u001b[0m\n","output_type":"stream"},{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"\" *smiling face* Attention has become an essential component in many Natural Language Processing (NLP) tasks, including the Transformer architecture. Self-attention layers allow models to focus on specific parts of input sequences and weigh their relevance when processing those sequences. It's like how you focus on certain regions while reading a long document; without attention, you might miss important details. In a Transformer model, these attention mechanisms are applied multiple times, much like how you might look back at previous sentences to better understand the context of the current one. *wink* Can I help you with more questions on this topic or maybe some other AI concepts? *hint hint*\""},"metadata":{}}]},{"cell_type":"markdown","source":"So you can see that now the prompt is a bit different. We have a system prompt and we have the system prompt template that we created earlier.\nAnd we have a human prompt with the category \"Generative AI\".\n\nSo you can see that this helps the get a bit more context on what has to be done.\nThe system prompt is clearly some context to help the understand what has to be done, and the human prompt is a question that is passed by the human.\n\nAnd you can see that the LLM is now providing a comma separated list of the given subcategories.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}