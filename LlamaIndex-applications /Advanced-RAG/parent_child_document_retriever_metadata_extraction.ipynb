{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akashmathur-2212/LLMs-playground/blob/main/LlamaIndex-applications%20/Advanced-RAG/parent_child_document_retriever_metadata_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "025f3e20-aec9-491c-8c90-234aed406a25",
      "metadata": {
        "id": "025f3e20-aec9-491c-8c90-234aed406a25"
      },
      "source": [
        "# Parent-Child Document Retriever with Metadata Extraction\n",
        "\n",
        "This notebook shows how you can use recursive retrieval to traverse node relationships and fetch nodes based on \"references\".\n",
        "\n",
        "When you first perform retrieval, you may want to retrieve the reference as opposed to the raw text. You can have multiple references point to the same node.\n",
        "\n",
        "In this guide we explore some different usages of node references:\n",
        "- **Chunk references**: Different chunk sizes referring to a bigger chunk\n",
        "- **Metadata references**: Summaries + Generated Questions referring to a bigger chunk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qqq llama-index llama-hub langchain openai accelerate==0.21.0 bitsandbytes==0.40.2 transformers sentence_transformers InstructorEmbedding chromadb"
      ],
      "metadata": {
        "id": "F8XBoiRQ9zQb"
      },
      "id": "F8XBoiRQ9zQb",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "273f38de-e79a-4ce2-ad4e-2c70afc33f34",
      "metadata": {
        "id": "273f38de-e79a-4ce2-ad4e-2c70afc33f34"
      },
      "source": [
        "## Setup\n",
        "\n",
        "1. In this section we will work with the QLoRA paper and create an initial set of nodes (chunk size 1024).\n",
        "2. We will use Open Source LLM [`zephyr-7b-alpha`](https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha) and embedding [`hkunlp/instructor-large`](https://huggingface.co/hkunlp/instructor-large)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "6cd97455-5ff3-43ee-8222-f496ec234dc7",
      "metadata": {
        "id": "6cd97455-5ff3-43ee-8222-f496ec234dc7"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "# transformers\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "# llama_index\n",
        "from llama_index.prompts import PromptTemplate\n",
        "from llama_index.llms import HuggingFaceLLM\n",
        "from llama_index import download_loader, Document, VectorStoreIndex, ServiceContext\n",
        "from llama_index.node_parser import SentenceSplitter\n",
        "from llama_index.schema import IndexNode\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "from llama_index.response.notebook_utils import display_source_node\n",
        "from llama_index.retrievers import RecursiveRetriever\n",
        "from llama_index.query_engine import RetrieverQueryEngine\n",
        "from llama_index.vector_stores import ChromaVectorStore\n",
        "from llama_index.storage.storage_context import StorageContext\n",
        "\n",
        "# Metadata Extraction\n",
        "from llama_index.extractors import (\n",
        "    SummaryExtractor,\n",
        "    QuestionsAnsweredExtractor,\n",
        ")\n",
        "\n",
        "# db\n",
        "import chromadb\n",
        "\n",
        "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data"
      ],
      "metadata": {
        "id": "cg0vTaLvBvYY"
      },
      "id": "cg0vTaLvBvYY"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a07c0e42-1ae8-4267-9355-6bb75323f82a",
      "metadata": {
        "id": "a07c0e42-1ae8-4267-9355-6bb75323f82a"
      },
      "outputs": [],
      "source": [
        "PDFReader = download_loader(\"PDFReader\")\n",
        "loader = PDFReader()\n",
        "docs = loader.load_data(file=Path(\"./QLoRa.pdf\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reLS97G1_E7g",
        "outputId": "968e9c45-6b28-49ce-d2c0-451ae1f8f4fc"
      },
      "id": "reLS97G1_E7g",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(id_='60349a1c-cb4e-42ef-9db1-e8cbdfc4c0dc', embedding=None, metadata={'page_label': '1', 'file_name': 'QLoRa.pdf'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='d5842650b3ec5f81bc436b0f768188fd29d9b209f1e08a472ec049643bba89d9', text='QL ORA: Efficient Finetuning of Quantized LLMs\\nTim Dettmers∗Artidoro Pagnoni∗Ari Holtzman\\nLuke Zettlemoyer\\nUniversity of Washington\\n{dettmers,artidoro,ahai,lsz}@cs.washington.edu\\nAbstract\\nWe present QLORA, an efficient finetuning approach that reduces memory us-\\nage enough to finetune a 65B parameter model on a single 48GB GPU while\\npreserving full 16-bit finetuning task performance. QLORAbackpropagates gradi-\\nents through a frozen, 4-bit quantized pretrained language model into Low Rank\\nAdapters (LoRA). Our best model family, which we name Guanaco , outperforms\\nall previous openly released models on the Vicuna benchmark, reaching 99.3%\\nof the performance level of ChatGPT while only requiring 24 hours of finetuning\\non a single GPU. QLORAintroduces a number of innovations to save memory\\nwithout sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that\\nis information theoretically optimal for normally distributed weights (b) Double\\nQuantization to reduce the average memory footprint by quantizing the quantization\\nconstants, and (c) Paged Optimizers to manage memory spikes. We use QLORA\\nto finetune more than 1,000 models, providing a detailed analysis of instruction\\nfollowing and chatbot performance across 8 instruction datasets, multiple model\\ntypes (LLaMA, T5), and model scales that would be infeasible to run with regular\\nfinetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA\\nfinetuning on a small high-quality dataset leads to state-of-the-art results, even\\nwhen using smaller models than the previous SoTA. We provide a detailed analysis\\nof chatbot performance based on both human and GPT-4 evaluations showing that\\nGPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Fur-\\nthermore, we find that current chatbot benchmarks are not trustworthy to accurately\\nevaluate the performance levels of chatbots. A lemon-picked analysis demonstrates\\nwhere Guanaco fails compared to ChatGPT. We release all of our models and code,\\nincluding CUDA kernels for 4-bit training.2\\n1 Introduction\\nFinetuning large language models (LLMs) is a highly effective way to improve their performance,\\n[40,62,43,61,59,37] and to add desirable or remove undesirable behaviors [ 43,2,4]. However,\\nfinetuning very large models is prohibitively expensive; regular 16-bit finetuning of a LLaMA 65B\\nparameter model [ 57] requires more than 780 GB of GPU memory. While recent quantization\\nmethods can reduce the memory footprint of LLMs [ 14,13,18,66], such techniques only work for\\ninference and break down during training [65].\\nWe demonstrate for the first time that it is possible to finetune a quantized 4-bit model without any\\nperformance degradation. Our method, QLORA, uses a novel high-precision technique to quantize\\na pretrained model to 4-bit, then adds a small set of learnable Low-rank Adapter weights [ 28]\\n∗Equal contribution.\\n2https://github.com/artidoro/qlora andhttps://github.com/TimDettmers/bitsandbytes\\nPreprint. Under review.arXiv:2305.14314v1  [cs.LG]  23 May 2023', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[0].get_content()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "hicGJdkV-9oh",
        "outputId": "9e2cb541-20a5-4f84-8854-0c3d22754ca2"
      },
      "id": "hicGJdkV-9oh",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'QL ORA: Efficient Finetuning of Quantized LLMs\\nTim Dettmers∗Artidoro Pagnoni∗Ari Holtzman\\nLuke Zettlemoyer\\nUniversity of Washington\\n{dettmers,artidoro,ahai,lsz}@cs.washington.edu\\nAbstract\\nWe present QLORA, an efficient finetuning approach that reduces memory us-\\nage enough to finetune a 65B parameter model on a single 48GB GPU while\\npreserving full 16-bit finetuning task performance. QLORAbackpropagates gradi-\\nents through a frozen, 4-bit quantized pretrained language model into Low Rank\\nAdapters (LoRA). Our best model family, which we name Guanaco , outperforms\\nall previous openly released models on the Vicuna benchmark, reaching 99.3%\\nof the performance level of ChatGPT while only requiring 24 hours of finetuning\\non a single GPU. QLORAintroduces a number of innovations to save memory\\nwithout sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that\\nis information theoretically optimal for normally distributed weights (b) Double\\nQuantization to reduce the average memory footprint by quantizing the quantization\\nconstants, and (c) Paged Optimizers to manage memory spikes. We use QLORA\\nto finetune more than 1,000 models, providing a detailed analysis of instruction\\nfollowing and chatbot performance across 8 instruction datasets, multiple model\\ntypes (LLaMA, T5), and model scales that would be infeasible to run with regular\\nfinetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA\\nfinetuning on a small high-quality dataset leads to state-of-the-art results, even\\nwhen using smaller models than the previous SoTA. We provide a detailed analysis\\nof chatbot performance based on both human and GPT-4 evaluations showing that\\nGPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Fur-\\nthermore, we find that current chatbot benchmarks are not trustworthy to accurately\\nevaluate the performance levels of chatbots. A lemon-picked analysis demonstrates\\nwhere Guanaco fails compared to ChatGPT. We release all of our models and code,\\nincluding CUDA kernels for 4-bit training.2\\n1 Introduction\\nFinetuning large language models (LLMs) is a highly effective way to improve their performance,\\n[40,62,43,61,59,37] and to add desirable or remove undesirable behaviors [ 43,2,4]. However,\\nfinetuning very large models is prohibitively expensive; regular 16-bit finetuning of a LLaMA 65B\\nparameter model [ 57] requires more than 780 GB of GPU memory. While recent quantization\\nmethods can reduce the memory footprint of LLMs [ 14,13,18,66], such techniques only work for\\ninference and break down during training [65].\\nWe demonstrate for the first time that it is possible to finetune a quantized 4-bit model without any\\nperformance degradation. Our method, QLORA, uses a novel high-precision technique to quantize\\na pretrained model to 4-bit, then adds a small set of learnable Low-rank Adapter weights [ 28]\\n∗Equal contribution.\\n2https://github.com/artidoro/qlora andhttps://github.com/TimDettmers/bitsandbytes\\nPreprint. Under review.arXiv:2305.14314v1  [cs.LG]  23 May 2023'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "493e5492-a6ae-4e3e-aa23-274c0605b165",
      "metadata": {
        "id": "493e5492-a6ae-4e3e-aa23-274c0605b165"
      },
      "outputs": [],
      "source": [
        "# combine all the text\n",
        "doc_text = \"\\n\\n\".join([d.get_content() for d in docs])\n",
        "documents = [Document(text=doc_text)]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chunking"
      ],
      "metadata": {
        "id": "QTsvxzzVCEB8"
      },
      "id": "QTsvxzzVCEB8"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "91b997ae-9260-4ae7-af2f-0f8d38625d32",
      "metadata": {
        "id": "91b997ae-9260-4ae7-af2f-0f8d38625d32"
      },
      "outputs": [],
      "source": [
        "node_parser = SentenceSplitter(chunk_size=1024)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "0cda44b0-fd27-4255-9aa7-08d358635772",
      "metadata": {
        "id": "0cda44b0-fd27-4255-9aa7-08d358635772"
      },
      "outputs": [],
      "source": [
        "base_nodes = node_parser.get_nodes_from_documents(documents)\n",
        "# set node ids to be a constant\n",
        "for idx, node in enumerate(base_nodes):\n",
        "    node.id_ = f\"node-{idx}\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print all the node ids corrosponding to all the chunks\n",
        "for node in base_nodes:\n",
        "  print(node.id_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lch2KK0-CNqs",
        "outputId": "07191401-9ca7-4b2d-8161-d9350fe29d07"
      },
      "id": "Lch2KK0-CNqs",
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "node-0\n",
            "node-1\n",
            "node-2\n",
            "node-3\n",
            "node-4\n",
            "node-5\n",
            "node-6\n",
            "node-7\n",
            "node-8\n",
            "node-9\n",
            "node-10\n",
            "node-11\n",
            "node-12\n",
            "node-13\n",
            "node-14\n",
            "node-15\n",
            "node-16\n",
            "node-17\n",
            "node-18\n",
            "node-19\n",
            "node-20\n",
            "node-21\n",
            "node-22\n",
            "node-23\n",
            "node-24\n",
            "node-25\n",
            "node-26\n",
            "node-27\n",
            "node-28\n",
            "node-29\n",
            "node-30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, the entire document is divided into 30 nodes."
      ],
      "metadata": {
        "id": "CGJs6N5HSsd0"
      },
      "id": "CGJs6N5HSsd0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM (`zephyr-7b-alpha`)"
      ],
      "metadata": {
        "id": "e0GawOxTDHuV"
      },
      "id": "e0GawOxTDHuV"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "# huggingface api token for downloading llama2\n",
        "hf_token = userdata.get('hf_token')\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "\n",
        "def messages_to_prompt(messages):\n",
        "  prompt = \"\"\n",
        "  for message in messages:\n",
        "    if message.role == 'system':\n",
        "      prompt += f\"<|system|>\\n{message.content}</s>\\n\"\n",
        "    elif message.role == 'user':\n",
        "      prompt += f\"<|user|>\\n{message.content}</s>\\n\"\n",
        "    elif message.role == 'assistant':\n",
        "      prompt += f\"<|assistant|>\\n{message.content}</s>\\n\"\n",
        "\n",
        "  # ensure we start with a system prompt, insert blank if needed\n",
        "  if not prompt.startswith(\"<|system|>\\n\"):\n",
        "    prompt = \"<|system|>\\n</s>\\n\" + prompt\n",
        "\n",
        "  # add final assistant prompt\n",
        "  prompt = prompt + \"<|assistant|>\\n\"\n",
        "\n",
        "  return prompt\n",
        "\n",
        "\n",
        "llm = HuggingFaceLLM(\n",
        "    model_name=\"HuggingFaceH4/zephyr-7b-alpha\",\n",
        "    tokenizer_name=\"HuggingFaceH4/zephyr-7b-alpha\",\n",
        "    query_wrapper_prompt=PromptTemplate(\"<|system|>\\n</s>\\n<|user|>\\n{query_str}</s>\\n<|assistant|>\\n\"),\n",
        "    context_window=3900,\n",
        "    max_new_tokens=256,\n",
        "    model_kwargs={\"quantization_config\": quantization_config},\n",
        "    # tokenizer_kwargs={},\n",
        "    generate_kwargs={\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95},\n",
        "    messages_to_prompt=messages_to_prompt,\n",
        "    device_map=\"auto\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "59329bc8fa5846f6bf7b47472554704e",
            "e69cd2a80e694ae386fda153ea41da87",
            "c3714ca613384126a7dc2522c5565444",
            "8deb050246744b698a9bcabadd15b203",
            "bfc160d260144a848f26348d033ed6b7",
            "e5831f1486bf43f7860a26d05e2699c6",
            "6718b533d6f24039b94fa17f4f8d8a66",
            "7075de14925547c7987b937d824cb317",
            "073f7e3ae9924b9c93ec4de08c3bbc3f",
            "84fafb68d4ea4f15831ecc695474af9c",
            "a8e2202bb7d74a87b83ee88f9b23b523"
          ]
        },
        "id": "Kfu5pGb7Chj4",
        "outputId": "f5b2911b-b581-49fd-a36c-e384855b8d21"
      },
      "id": "Kfu5pGb7Chj4",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "59329bc8fa5846f6bf7b47472554704e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding (`hkunlp/instructor-large`)"
      ],
      "metadata": {
        "id": "X3m9X11PDefF"
      },
      "id": "X3m9X11PDefF"
    },
    {
      "cell_type": "code",
      "source": [
        "embed_model = HuggingFaceInstructEmbeddings(\n",
        "    model_name=\"hkunlp/instructor-large\", model_kwargs={\"device\": DEVICE}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJmrabJYDmPW",
        "outputId": "b32a4da5-26dd-454b-f184-e478b70eebce"
      },
      "id": "OJmrabJYDmPW",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load INSTRUCTOR_Transformer\n",
            "max_seq_length  512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "38e47623-b67d-45d6-9b24-33ba84719f1f",
      "metadata": {
        "id": "38e47623-b67d-45d6-9b24-33ba84719f1f"
      },
      "outputs": [],
      "source": [
        "# set your ServiceContext for all the next steps\n",
        "service_context = ServiceContext.from_defaults(\n",
        "    llm=llm, embed_model=embed_model\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f43ebab2-fc46-41ea-8a92-9148994d793f",
      "metadata": {
        "id": "f43ebab2-fc46-41ea-8a92-9148994d793f"
      },
      "source": [
        "## Baseline Retriever\n",
        "\n",
        "Define a baseline retriever that simply fetches the top-k raw text nodes by embedding similarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "704fb3da-710e-4ad9-b630-565911917f0c",
      "metadata": {
        "id": "704fb3da-710e-4ad9-b630-565911917f0c"
      },
      "outputs": [],
      "source": [
        "base_index = VectorStoreIndex(base_nodes, service_context=service_context)\n",
        "base_retriever = base_index.as_retriever(similarity_top_k=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "160c339b-601a-486b-9e17-dd6cc9f133ea",
      "metadata": {
        "id": "160c339b-601a-486b-9e17-dd6cc9f133ea"
      },
      "outputs": [],
      "source": [
        "retrievals = base_retriever.retrieve(\n",
        "    \"Can you tell me about the Paged Optimizers?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "632610f3-c8f2-440a-ab27-5ca7d65f882a",
      "metadata": {
        "id": "632610f3-c8f2-440a-ab27-5ca7d65f882a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "outputId": "141ba43b-93d3-426c-ff05-47006c46e870"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Node ID:** node-5<br>**Similarity:** 0.8639193985783434<br>**Text:** On average, for a blocksize of 64, this quantization reduces the memory footprint per\nparameter from 32/64 = 0 .5bits, to 8/64 + 32 /(64·256) = 0 .127bits, a reduction of 0.373 bits\nper parameter.\nPaged Optimizers use the NVIDIA unified memory3feature wich does automatic page-to-page\ntransfers between the CPU and GPU for error-free GPU processing in the scenario where the GPU\noccasionally runs out-of-memory. The feature works like regular memory paging between CPU RAM\nand the disk. We use this feature to allocate paged memory for the optimizer states which are then\nautomatically evicted to CPU RAM when the GPU runs out-of-memory and paged back into GPU\nmemory when the memory is needed in the optimizer update step.\nQL ORA.Using the components described above, we define QLORAfor a single linear layer in\nthe quantized base model with a single LoRA adapter as follows:\nYBF16=XBF16doubleDequant (cFP32\n1, ck-bit\n2,WNF4) +XBF16LBF16\n1LBF16\n2, (5)\nwhere doubleDequant (·)is defined as:\ndoubleDequant (cFP32\n1, ck-bit\n2,Wk-bit) =dequant (dequant (cFP32\n1, ck-bit\n2),W4bit) =WBF16,(6)\nWe use NF4 for Wand FP8 for c2. We use a blocksize of 64 for Wfor higher quantization precision\nand a blocksize of 256 for c2to conserve memory.\nFor parameter updates only the gradient with respect to the error for the adapters weights∂E\n∂Liare\nneeded, and not for 4-bit weights∂E\n∂W. However, the calculation of∂E\n∂Lientails the calculation of∂X\n∂W\nwhich proceeds via equation (5) with dequantization from st...<br>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Node ID:** node-6<br>**Similarity:** 0.8246144101148492<br>**Text:** We provide more details in the results section for each particular setup to make the results more\nreadable. Full details in Appendix A.\nQLoRA-AllQLoRA-FFN\nQLoRA-AttentionAlpaca (ours)\nStanford-Alpaca\nModel6061626364RougeL\nbits\n4\n16\nFigure 2: RougeL for LLaMA 7B models on the\nAlpaca dataset. Each point represents a run with a\ndifferent random seed. We improve on the Stanford\nAlpaca fully finetuned default hyperparameters to\nconstruct a strong 16-bit baseline for comparisons.\nUsing LoRA on all transformer layers is critical to\nmatch 16-bit performance.While paged optimizers are critical to do 33B/65B\nQLORAtuning on a single 24/48GB GPU, we do\nnot provide hard measurements for Paged Optimiz-\ners since the paging only occurs when processing\nmini-batches with long sequence lengths, which is\nrare. We do, however, perform an analysis of the\nruntime of paged optimizers for 65B models on\n48GB GPUs and find that with a batch size of 16,\npaged optimizers provide the same training speed\nas regular optimizers. Future work should measure\nand characterize under what circumstances slow-\ndowns occur from the paging process.\nDefault LoRA hyperparameters do not match 16-\nbit performance When using the standard prac-\ntice of applying LoRA to query and value attention\nprojection matrices [ 28], we are not able to replicate\nfull finetuning performance for large base models.\nAs shown in Figure 2 for LLaMA 7B finetuning on\nAlpaca, we find that the most critical LoRA hyper-\nparameter is how many L...<br>"
          },
          "metadata": {}
        }
      ],
      "source": [
        "for n in retrievals:\n",
        "    display_source_node(n, source_length=1500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "96dd8a01-1cae-4614-beab-5b5e0434fefe",
      "metadata": {
        "id": "96dd8a01-1cae-4614-beab-5b5e0434fefe"
      },
      "outputs": [],
      "source": [
        "query_engine_base = RetrieverQueryEngine.from_args(\n",
        "    base_retriever, service_context=service_context\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "82ae66ff-7d12-45c8-9b1a-adb20bd3c7ea",
      "metadata": {
        "id": "82ae66ff-7d12-45c8-9b1a-adb20bd3c7ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05b68e09-7fdd-45e9-b7d2-20ab8ef63611"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paged Optimizers are a feature used in QLoRA to allocate paged memory for optimizer states. This allows for automatic page-to-page transfers between CPU and GPU memory for error-free GPU processing when the GPU runs out of memory. This feature works like regular memory paging between CPU RAM and disk. By using this feature, QLoRA can significantly reduce the required memory for finetuning models. However, the paging process only occurs when processing mini-batches with long sequence lengths, which is rare. The default LoRA hyperparameters do not match 16-bit performance, and the most critical LoRA hyperparameter is the number of LoRA adapters used in total. LoRA on all linear transformer block layers is required to match full finetuning performance. NormalFloat data type significantly improves bit-for-bit accuracy gains compared to regular 4-bit Floats, and double quantization allows for a more fine-grained control over the memory footprint to fit models of certain size into certain GPUs.\n"
          ]
        }
      ],
      "source": [
        "response = query_engine_base.query(\n",
        "    \"Can you tell me about the Paged Optimizers?\"\n",
        ")\n",
        "print(str(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5431df3-d255-4492-bce4-bbebde6f2306",
      "metadata": {
        "id": "d5431df3-d255-4492-bce4-bbebde6f2306"
      },
      "source": [
        "## Chunk References: Smaller Child Chunks Referring to Bigger Parent Chunk\n",
        "\n",
        "Now, we will build smaller chunks that will point to their bigger parent chunks.\n",
        "\n",
        "During query-time, we retrieve smaller chunks, but we follow references to bigger chunks. This allows us to have more context for synthesis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "49c784d8-71e6-42bc-84d9-a2aea4217b8b",
      "metadata": {
        "id": "49c784d8-71e6-42bc-84d9-a2aea4217b8b"
      },
      "outputs": [],
      "source": [
        "sub_chunk_sizes = [256, 512]\n",
        "sub_node_parsers = [SentenceSplitter(chunk_size=c) for c in sub_chunk_sizes]\n",
        "\n",
        "all_nodes = []\n",
        "for base_node in base_nodes:\n",
        "    for n in sub_node_parsers:\n",
        "        sub_nodes = n.get_nodes_from_documents([base_node])\n",
        "        sub_inodes = [\n",
        "            IndexNode.from_text_node(sn, base_node.node_id) for sn in sub_nodes\n",
        "        ]\n",
        "        all_nodes.extend(sub_inodes)\n",
        "\n",
        "    # also add original node to node\n",
        "    original_node = IndexNode.from_text_node(base_node, base_node.node_id)\n",
        "    all_nodes.append(original_node)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "2d614088-b122-40ad-811a-29cc0c2a295e",
      "metadata": {
        "id": "2d614088-b122-40ad-811a-29cc0c2a295e"
      },
      "outputs": [],
      "source": [
        "all_nodes_dict = {n.node_id: n for n in all_nodes}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_nodes_dict.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9oJp0mv6EEWV",
        "outputId": "2c63ace8-feee-43fd-8b6f-043a51a714a1"
      },
      "id": "9oJp0mv6EEWV",
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['node-0', 'node-1', 'node-2', 'node-3', 'node-4', 'node-5', 'node-6', 'node-7', 'node-8', 'node-9', 'node-10', 'node-11', 'node-12', 'node-13', 'node-14', 'node-15', 'node-16', 'node-17', 'node-18', 'node-19', 'node-20', 'node-21', 'node-22', 'node-23', 'node-24', 'node-25', 'node-26', 'node-27', 'node-28', 'node-29', 'node-30', 'aff4cd7a-114d-4ae2-a50e-ca5d30afc9ce', '756bcf23-c37a-4ab6-b1ce-1e0a87992845', '98453e39-b13b-499e-8cc9-81b9be409191', '6cb1624d-b628-4f2e-901c-d0fb5d495707', '641b49f2-0494-49c3-a013-c220a29a19f0', 'e5ecbc64-8b29-4e34-83ef-aa5162180da0', 'a9293f16-2787-4199-939c-22bfca2b9958', 'db2f9d01-62f9-40d3-88a6-fb62dd8a832e', 'e2f9f48f-b4a0-4d19-8317-bcd60d88aefd', '970443fa-3aea-4d06-adb7-dcd6651bb6de', '907b2e47-825c-44af-a2cf-8c41c89e3960', '95d2bf7e-938c-44e8-9e0b-99462268880a', '3689fecb-8967-4928-b194-c82dc1a86736', 'c9c94976-987c-4100-8a6f-573099e4c4bc', 'aaece5c8-2df4-4b68-8fd7-b8db966eb6cf', 'f093581b-8178-499c-81b5-2750d0ac2b0d', '77e14b4b-5b72-4ae8-9738-194167679d0c', '79f9383c-f447-4737-9b10-7d3764fafe92', 'e36cc6f0-bfb1-4758-9eab-6c9504ab1011', 'e5597d77-0b9d-4038-bf30-d98046e3df09', '2d5fa69a-c23a-40fc-8223-a56cba47fe2c', '54babdea-5c89-4c44-9606-26046cd1a516', 'b9ea10c2-928c-4a1f-a441-f14030ccb952', 'f1fbc107-64c4-443e-a23c-391780911230', '570d7006-3f10-44c2-8329-83965b4d31a1', 'daf31574-5169-43ec-9663-c68e5f997e9d', '4e8a7a85-29d1-418c-a072-c8fe356545cc', '78e28ab4-9d36-4ec0-a6a7-4cb50475043d', 'd923a476-beef-4a2d-b7ec-7ba2262f5377', '278bd773-fa13-4448-b590-aa7b71f5f03c', '624f4e54-86c8-45b0-a6e1-9717a7bc6741'])"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_nodes_dict['aff4cd7a-114d-4ae2-a50e-ca5d30afc9ce']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X15pzmycEo_l",
        "outputId": "504c811b-bc6b-4eea-df4d-491884ef4464"
      },
      "id": "X15pzmycEo_l",
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "IndexNode(id_='aff4cd7a-114d-4ae2-a50e-ca5d30afc9ce', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='25b08fc5a0708dcadb67fb32123722c3e726452fc10252d1ea1179f3fc6e20c6', text='Question: How does QLORA, an efficient finetuning approach, reduce memory usage while preserving full 16-bit finetuning task performance for LLMs? What innovations does it introduce to achieve this?', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n', index_id='node-0')"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# all_nodes_list = list(all_nodes_dict.keys())\n",
        "# index_id = [x for x in all_nodes_list if \"node-\" not in x]\n",
        "# for id in index_id:\n",
        "#   print(f\"{id} ---> {all_nodes_dict[id].index_id}\")\n",
        "#   print(\"-\"*40, end=\"\\n\")"
      ],
      "metadata": {
        "id": "fU-ZSXG5FSpQ"
      },
      "id": "fU-ZSXG5FSpQ",
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "See that these many smaller chunks (`IndexNode`) are associated with each of the original text chunks(`TextNode`) for example `node-0`. In fact, all of the smaller chunks reference to the large chunk in the metadata with `index_id` pointing to the index ID of the larger chunk."
      ],
      "metadata": {
        "id": "POcAQyP9TXug"
      },
      "id": "POcAQyP9TXug"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Index from these smaller chunks (IndexNode)"
      ],
      "metadata": {
        "id": "J7-QiFF6G5YP"
      },
      "id": "J7-QiFF6G5YP"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "a44ef2d5-0342-4073-831f-f35dd6f04dc0",
      "metadata": {
        "id": "a44ef2d5-0342-4073-831f-f35dd6f04dc0"
      },
      "outputs": [],
      "source": [
        "vector_index_chunk = VectorStoreIndex(\n",
        "    all_nodes, service_context=service_context\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "c06af99f-02be-4055-a6ea-3071ffe8fc8a",
      "metadata": {
        "id": "c06af99f-02be-4055-a6ea-3071ffe8fc8a"
      },
      "outputs": [],
      "source": [
        "vector_retriever_chunk = vector_index_chunk.as_retriever(similarity_top_k=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we perform retrieval, we want to retrieve the reference as opposed to the raw text. You can have multiple references point to the same node."
      ],
      "metadata": {
        "id": "DCm2lt6HHJ7B"
      },
      "id": "DCm2lt6HHJ7B"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "4c7c5e43-45b5-42d6-afc5-cb81ed3cb211",
      "metadata": {
        "id": "4c7c5e43-45b5-42d6-afc5-cb81ed3cb211"
      },
      "outputs": [],
      "source": [
        "retriever_chunk = RecursiveRetriever(\n",
        "    \"vector\",\n",
        "    retriever_dict={\"vector\": vector_retriever_chunk},\n",
        "    node_dict=all_nodes_dict,\n",
        "    verbose=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "9e9f7bcb-5442-4d2d-a7eb-814b68ebb45c",
      "metadata": {
        "id": "9e9f7bcb-5442-4d2d-a7eb-814b68ebb45c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "outputId": "a4c76ee0-4e66-430f-cfa3-4b72d2e43610"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;34mRetrieving with query id None: Can you tell me about the Paged Optimizers?\n",
            "\u001b[0m\u001b[1;3;38;5;200mRetrieved node with id, entering: node-4\n",
            "\u001b[0m\u001b[1;3;34mRetrieving with query id node-4: Can you tell me about the Paged Optimizers?\n",
            "\u001b[0m\u001b[1;3;38;5;200mRetrieved node with id, entering: node-5\n",
            "\u001b[0m\u001b[1;3;34mRetrieving with query id node-5: Can you tell me about the Paged Optimizers?\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Node ID:** node-4<br>**Similarity:** 0.8935850984244503<br>**Text:** For our data type, we\nset the arbitrary range [−1,1]. As such, both the quantiles for the data type and the neural network\nweights need to be normalized into this range.\nThe information theoretically optimal data type for zero-mean normal distributions with arbitrary\nstandard deviations σin the range [−1,1]is computed as follows: (1) estimate the 2k+ 1quantiles\nof a theoretical N(0,1)distribution to obtain a k-bit quantile quantization data type for normal distri-\nbutions, (2) take this data type and normalize its values into the [−1,1]range, (3) quantize an input\nweight tensor by normalizing it into the [−1,1]range through absolute maximum rescaling.\nOnce the weight range and data type range match, we can quantize as usual. Step (3) is equivalent to\nrescaling the standard deviation of the weight tensor to match the standard deviation of the k-bit data\ntype. More formally, we estimate the 2kvalues qiof the data type as follows:\nqi=1\n2\u0012\nQX\u0012i\n2k+ 1\u0013\n+QX\u0012i+ 1\n2k+ 1\u0013\u0013\n, (4)\nwhere QX(·)is the quantile function of the standard normal distribution N(0,1). A problem for\na symmetric k-bit quantization is that this approach does not have an exact representation of zero,\nwhich is an important property to quantize padding and other zero-valued elements with no error. To\n4\n\nensure a discrete zeropoint of 0and to use all 2kbits for a k-bit datatype, we create an asymmetric\ndata type by estimating the quantiles qiof two ranges qi:2k−1for the negative part and 2k−1+ 1for\nthe positive part and then we unify these sets of qiand remove one of the two zeros that occurs in both\nsets. We term the resulting data type that has equal expected number of values in each quantization bin\nk-bit NormalFloat (NFk), since the data type is information-theoretically optimal for zero-centered\nnormally distributed data. The exact values of this data type can be found in Appendix E.\nDouble Quantization We introduce Double Quantization (DQ), the process of quantizing the\nquantization constants for add...<br>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Node ID:** node-5<br>**Similarity:** 0.8929982170962448<br>**Text:** On average, for a blocksize of 64, this quantization reduces the memory footprint per\nparameter from 32/64 = 0 .5bits, to 8/64 + 32 /(64·256) = 0 .127bits, a reduction of 0.373 bits\nper parameter.\nPaged Optimizers use the NVIDIA unified memory3feature wich does automatic page-to-page\ntransfers between the CPU and GPU for error-free GPU processing in the scenario where the GPU\noccasionally runs out-of-memory. The feature works like regular memory paging between CPU RAM\nand the disk. We use this feature to allocate paged memory for the optimizer states which are then\nautomatically evicted to CPU RAM when the GPU runs out-of-memory and paged back into GPU\nmemory when the memory is needed in the optimizer update step.\nQL ORA.Using the components described above, we define QLORAfor a single linear layer in\nthe quantized base model with a single LoRA adapter as follows:\nYBF16=XBF16doubleDequant (cFP32\n1, ck-bit\n2,WNF4) +XBF16LBF16\n1LBF16\n2, (5)\nwhere doubleDequant (·)is defined as:\ndoubleDequant (cFP32\n1, ck-bit\n2,Wk-bit) =dequant (dequant (cFP32\n1, ck-bit\n2),W4bit) =WBF16,(6)\nWe use NF4 for Wand FP8 for c2. We use a blocksize of 64 for Wfor higher quantization precision\nand a blocksize of 256 for c2to conserve memory.\nFor parameter updates only the gradient with respect to the error for the adapters weights∂E\n∂Liare\nneeded, and not for 4-bit weights∂E\n∂W. However, the calculation of∂E\n∂Lientails the calculation of∂X\n∂W\nwhich proceeds via equation (5) with dequantization from storage WNF4to computation data type\nWBF16to calculate the derivative∂X\n∂Win BFloat16 precision.\nTo summarize, QLORAhas one storage data type (usually 4-bit NormalFloat) and a computation\ndata type (16-bit BrainFloat). We dequantize the storage data type to the computation data type\nto perform the forward and backward pass, but we only compute weight gradients for the LoRA\nparameters which use 16-bit BrainFloat.\n4 QLoRA vs. Standard Finetuning\nWe have discussed how QLoRA works and how it can signi...<br>"
          },
          "metadata": {}
        }
      ],
      "source": [
        "nodes = retriever_chunk.retrieve(\n",
        "    \"Can you tell me about the Paged Optimizers?\"\n",
        ")\n",
        "for node in nodes:\n",
        "    display_source_node(node, source_length=2000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "411f26ad-d13b-4858-938e-efcfa899e8cd",
      "metadata": {
        "id": "411f26ad-d13b-4858-938e-efcfa899e8cd"
      },
      "outputs": [],
      "source": [
        "query_engine_chunk = RetrieverQueryEngine.from_args(\n",
        "    retriever_chunk, service_context=service_context\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "4cd98366-0d5f-4d04-87cd-b811990b7485",
      "metadata": {
        "id": "4cd98366-0d5f-4d04-87cd-b811990b7485",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8195945d-9ee8-48d6-a5d4-493894e4c97e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;34mRetrieving with query id None: Can you tell me about the Paged Optimizers?\n",
            "\u001b[0m\u001b[1;3;38;5;200mRetrieved node with id, entering: node-4\n",
            "\u001b[0m\u001b[1;3;34mRetrieving with query id node-4: Can you tell me about the Paged Optimizers?\n",
            "\u001b[0m\u001b[1;3;38;5;200mRetrieved node with id, entering: node-5\n",
            "\u001b[0m\u001b[1;3;34mRetrieving with query id node-5: Can you tell me about the Paged Optimizers?\n",
            "\u001b[0mYes, according to the given context information, Paged Optimizers are a feature used in the QLoRA model to allocate paged memory for optimizer states. This feature automatically transfers memory between the CPU and GPU when the GPU runs out of memory, allowing for error-free GPU processing. This is done by using the NVIDIA unified memory feature, which works like regular memory paging between CPU RAM and the disk. This allows for more efficient use of memory during the optimization process.\n"
          ]
        }
      ],
      "source": [
        "response = query_engine_chunk.query(\n",
        "    \"Can you tell me about the Paged Optimizers?\"\n",
        ")\n",
        "print(str(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bcc7379-c077-40b7-ba4e-f47f80def0c7",
      "metadata": {
        "id": "3bcc7379-c077-40b7-ba4e-f47f80def0c7"
      },
      "source": [
        "## Metadata References: Summaries + Generated Questions referring to a bigger chunk\n",
        "\n",
        "Now, we will add some additional context that references the source node.\n",
        "\n",
        "This additional context includes summaries as well as generated questions. `Due to the limited compute I am only extracting questions, but you can uncomment the summarizer to extract summaries.`\n",
        "\n",
        "During query-time, we retrieve smaller chunks, but we follow references to bigger chunks. This allows us to have more context for synthesis."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "VSV8jys5WeO8"
      },
      "id": "VSV8jys5WeO8",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "5c5d6f87-790e-4b82-abb2-cc6944678b00",
      "metadata": {
        "id": "5c5d6f87-790e-4b82-abb2-cc6944678b00"
      },
      "outputs": [],
      "source": [
        "extractors = [\n",
        "    # SummaryExtractor(summaries=[\"self\"], llm=llm, show_progress=True),\n",
        "    QuestionsAnsweredExtractor(questions=1, llm=llm, show_progress=True),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "e47c706c-940e-499d-b742-eaf09a230b0d",
      "metadata": {
        "id": "e47c706c-940e-499d-b742-eaf09a230b0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3020458f-8a32-490e-fe26-6c34c642b511"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 31/31 [04:24<00:00,  8.52s/it]  \n"
          ]
        }
      ],
      "source": [
        "# run metadata extractor across base nodes, get back dictionaries\n",
        "metadata_dicts = []\n",
        "for extractor in extractors:\n",
        "    metadata_dicts.extend(extractor.extract(base_nodes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "f18d2109-5fcb-4fd5-b147-23897fed8787",
      "metadata": {
        "id": "f18d2109-5fcb-4fd5-b147-23897fed8787"
      },
      "outputs": [],
      "source": [
        "# all nodes consists of source nodes, along with metadata\n",
        "import copy\n",
        "\n",
        "all_nodes = copy.deepcopy(base_nodes)\n",
        "for idx, d in enumerate(metadata_dicts):\n",
        "    inode_q = IndexNode(\n",
        "        text=d[\"questions_this_excerpt_can_answer\"],\n",
        "        index_id=base_nodes[idx].node_id,\n",
        "    )\n",
        "    # inode_s = IndexNode(\n",
        "    #     text=d[\"section_summary\"], index_id=base_nodes[idx].node_id)\n",
        "    all_nodes.extend([inode_q]) #, inode_s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "8f90ada6-0969-40cc-a4ec-3579b4900cdd",
      "metadata": {
        "id": "8f90ada6-0969-40cc-a4ec-3579b4900cdd"
      },
      "outputs": [],
      "source": [
        "all_nodes_dict = {n.node_id: n for n in all_nodes}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector_index_metadata = VectorStoreIndex(all_nodes, service_context=service_context)\n",
        "vector_retriever_metadata = vector_index_metadata.as_retriever(similarity_top_k=2)"
      ],
      "metadata": {
        "id": "iEL31kfc5Fnp"
      },
      "id": "iEL31kfc5Fnp",
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "37ae791f-c183-4ad4-9a3a-253288ded5a7",
      "metadata": {
        "id": "37ae791f-c183-4ad4-9a3a-253288ded5a7"
      },
      "outputs": [],
      "source": [
        "retriever_metadata = RecursiveRetriever(\n",
        "    \"vector\",\n",
        "    retriever_dict={\"vector\": vector_retriever_metadata},\n",
        "    node_dict=all_nodes_dict,\n",
        "    verbose=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "3cd85685-19eb-44cc-ad27-1d163eaddad6",
      "metadata": {
        "id": "3cd85685-19eb-44cc-ad27-1d163eaddad6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3e84b9c4-85d6-4903-ada5-bfd81a237671"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;34mRetrieving with query id None: Can you tell me about the Paged Optimizers?\n",
            "\u001b[0m\u001b[1;3;38;5;200mRetrieving text node: On average, for a blocksize of 64, this quantization reduces the memory footprint per\n",
            "parameter from 32/64 = 0 .5bits, to 8/64 + 32 /(64·256) = 0 .127bits, a reduction of 0.373 bits\n",
            "per parameter.\n",
            "Paged Optimizers use the NVIDIA unified memory3feature wich does automatic page-to-page\n",
            "transfers between the CPU and GPU for error-free GPU processing in the scenario where the GPU\n",
            "occasionally runs out-of-memory. The feature works like regular memory paging between CPU RAM\n",
            "and the disk. We use this feature to allocate paged memory for the optimizer states which are then\n",
            "automatically evicted to CPU RAM when the GPU runs out-of-memory and paged back into GPU\n",
            "memory when the memory is needed in the optimizer update step.\n",
            "QL ORA.Using the components described above, we define QLORAfor a single linear layer in\n",
            "the quantized base model with a single LoRA adapter as follows:\n",
            "YBF16=XBF16doubleDequant (cFP32\n",
            "1, ck-bit\n",
            "2,WNF4) +XBF16LBF16\n",
            "1LBF16\n",
            "2, (5)\n",
            "where doubleDequant (·)is defined as:\n",
            "doubleDequant (cFP32\n",
            "1, ck-bit\n",
            "2,Wk-bit) =dequant (dequant (cFP32\n",
            "1, ck-bit\n",
            "2),W4bit) =WBF16,(6)\n",
            "We use NF4 for Wand FP8 for c2. We use a blocksize of 64 for Wfor higher quantization precision\n",
            "and a blocksize of 256 for c2to conserve memory.\n",
            "For parameter updates only the gradient with respect to the error for the adapters weights∂E\n",
            "∂Liare\n",
            "needed, and not for 4-bit weights∂E\n",
            "∂W. However, the calculation of∂E\n",
            "∂Lientails the calculation of∂X\n",
            "∂W\n",
            "which proceeds via equation (5) with dequantization from storage WNF4to computation data type\n",
            "WBF16to calculate the derivative∂X\n",
            "∂Win BFloat16 precision.\n",
            "To summarize, QLORAhas one storage data type (usually 4-bit NormalFloat) and a computation\n",
            "data type (16-bit BrainFloat). We dequantize the storage data type to the computation data type\n",
            "to perform the forward and backward pass, but we only compute weight gradients for the LoRA\n",
            "parameters which use 16-bit BrainFloat.\n",
            "4 QLoRA vs. Standard Finetuning\n",
            "We have discussed how QLoRA works and how it can significantly reduce the required memory for\n",
            "finetuning models. The main question now is whether QLoRA can perform as well as full-model\n",
            "finetuning. Furthermore, we want to analyze the components of QLoRA including the impact of\n",
            "NormalFloat4 over standard Float4. The following sections will discuss the experiments that aimed\n",
            "at answering these questions.\n",
            "3https://docs.nvidia.com/cuda/cuda-c-programming-guide\n",
            "5\n",
            "\n",
            "Experimental setup. We consider three architectures (encoder, encoder-decoder, and decoder only)\n",
            "and compare QLoRA with 16-bit adapter-finetuning and with full-finetuning for models up to 3B. Our\n",
            "evaluations include GLUE [ 58] with RoBERTa-large [ 38], Super-NaturalInstructions (TKInstruct)\n",
            "[61] with T5 [ 49], and 5-shot MMLU [ 24] after finetuning LLaMA on Flan v2 [ 39] and Alpaca\n",
            "[55]. To additionally study the advantages of NF4 over other 4-bit data types, we use the setup of\n",
            "Dettmers and Zettlemoyer [13] and measure post-quantization zero-shot accuracy and perplexity\n",
            "across different models (OPT [ 72], LLaMA [ 57], BLOOM [ 52], Pythia [ 7]) for model sizes 125m -\n",
            "13B. We provide more details in the results section for each particular setup to make the results more\n",
            "readable. Full details in Appendix A.\n",
            "QLoRA-AllQLoRA-FFN\n",
            "QLoRA-AttentionAlpaca (ours)\n",
            "Stanford-Alpaca\n",
            "Model6061626364RougeL\n",
            "bits\n",
            "4\n",
            "16\n",
            "Figure 2: RougeL for LLaMA 7B models on the\n",
            "Alpaca dataset. Each point represents a run with a\n",
            "different random seed. We improve on the Stanford\n",
            "Alpaca fully finetuned default hyperparameters to\n",
            "construct a strong 16-bit baseline for comparisons.\n",
            "\u001b[0m\u001b[1;3;38;5;200mRetrieved node with id, entering: node-0\n",
            "\u001b[0m\u001b[1;3;34mRetrieving with query id node-0: Can you tell me about the Paged Optimizers?\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Node ID:** node-5<br>**Similarity:** 0.8639193985783434<br>**Text:** On average, for a blocksize of 64, this quantization reduces the memory footprint per\nparameter from 32/64 = 0 .5bits, to 8/64 + 32 /(64·256) = 0 .127bits, a reduction of 0.373 bits\nper parameter.\nPaged Optimizers use the NVIDIA unified memory3feature wich does automatic page-to-page\ntransfers between the CPU and GPU for error-free GPU processing in the scenario where the GPU\noccasionally runs out-of-memory. The feature works like regular memory paging between CPU RAM\nand the disk. We use this feature to allocate paged memory for the optimizer states which are then\nautomatically evicted to CPU RAM when the GPU runs out-of-memory and paged back into GPU\nmemory when the memory is needed in the optimizer update step.\nQL ORA.Using the components described above, we define QLORAfor a single linear layer in\nthe quantized base model with a single LoRA adapter as follows:\nYBF16=XBF16doubleDequant (cFP32\n1, ck-bit\n2,WNF4) +XBF16LBF16\n1LBF16\n2, (5)\nwhere doubleDequant (·)is defined as:\ndoubleDequant (cFP32\n1, ck-bit\n2,Wk-bit) =dequant (dequant (cFP32\n1, ck-bit\n2),W4bit) =WBF16,(6)\nWe use NF4 for Wand FP8 for c2. We use a blocksize of 64 for Wfor higher quantization precision\nand a blocksize of 256 for c2to conserve memory.\nFor parameter updates only the gradient with respect to the error for the adapters weights∂E\n∂Liare\nneeded, and not for 4-bit weights∂E\n∂W. However, the calculation of∂E\n∂Lientails the calculation of∂X\n∂W\nwhich proceeds via equation (5) with dequantization from storage WNF4to computation data type\nWBF16to calculate the derivative∂X\n∂Win BFloat16 precision.\nTo summarize, QLORAhas one storage data type (usually 4-bit NormalFloat) and a computation\ndata type (16-bit BrainFloat). We dequantize the storage data type to the computation data type\nto perform the forward and backward pass, but we only compute weight gradients for the LoRA\nparameters which use 16-bit BrainFloat.\n4 QLoRA vs. Standard Finetuning\nWe have discussed how QLoRA works and how it can signi...<br>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Node ID:** node-0<br>**Similarity:** 0.840191491716247<br>**Text:** QL ORA: Efficient Finetuning of Quantized LLMs\nTim Dettmers∗Artidoro Pagnoni∗Ari Holtzman\nLuke Zettlemoyer\nUniversity of Washington\n{dettmers,artidoro,ahai,lsz}@cs.washington.edu\nAbstract\nWe present QLORA, an efficient finetuning approach that reduces memory us-\nage enough to finetune a 65B parameter model on a single 48GB GPU while\npreserving full 16-bit finetuning task performance. QLORAbackpropagates gradi-\nents through a frozen, 4-bit quantized pretrained language model into Low Rank\nAdapters (LoRA). Our best model family, which we name Guanaco , outperforms\nall previous openly released models on the Vicuna benchmark, reaching 99.3%\nof the performance level of ChatGPT while only requiring 24 hours of finetuning\non a single GPU. QLORAintroduces a number of innovations to save memory\nwithout sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that\nis information theoretically optimal for normally distributed weights (b) Double\nQuantization to reduce the average memory footprint by quantizing the quantization\nconstants, and (c) Paged Optimizers to manage memory spikes. We use QLORA\nto finetune more than 1,000 models, providing a detailed analysis of instruction\nfollowing and chatbot performance across 8 instruction datasets, multiple model\ntypes (LLaMA, T5), and model scales that would be infeasible to run with regular\nfinetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA\nfinetuning on a small high-quality dataset leads to state-of-the-art results, even\nwhen using smaller models than the previous SoTA. We provide a detailed analysis\nof chatbot performance based on both human and GPT-4 evaluations showing that\nGPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Fur-\nthermore, we find that current chatbot benchmarks are not trustworthy to accurately\nevaluate the performance levels of chatbots. A lemon-picked analysis demonstrates\nwhere Guanaco fails compared to ChatGPT. We release all of our models ...<br>"
          },
          "metadata": {}
        }
      ],
      "source": [
        "nodes = retriever_metadata.retrieve(\n",
        "    \"Can you tell me about the Paged Optimizers?\"\n",
        ")\n",
        "for node in nodes:\n",
        "    display_source_node(node, source_length=2000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "5285854a-69a6-4bc4-a2a5-1004cc790a63",
      "metadata": {
        "id": "5285854a-69a6-4bc4-a2a5-1004cc790a63"
      },
      "outputs": [],
      "source": [
        "query_engine_metadata = RetrieverQueryEngine.from_args(\n",
        "    retriever_metadata, service_context=service_context\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "4e0ada5c-9a83-4517-bbb7-899d4415d68a",
      "metadata": {
        "id": "4e0ada5c-9a83-4517-bbb7-899d4415d68a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7483de8-6094-4560-bb36-8b155c73764c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;34mRetrieving with query id None: Can you tell me about the Paged Optimizers?\n",
            "\u001b[0m\u001b[1;3;38;5;200mRetrieving text node: On average, for a blocksize of 64, this quantization reduces the memory footprint per\n",
            "parameter from 32/64 = 0 .5bits, to 8/64 + 32 /(64·256) = 0 .127bits, a reduction of 0.373 bits\n",
            "per parameter.\n",
            "Paged Optimizers use the NVIDIA unified memory3feature wich does automatic page-to-page\n",
            "transfers between the CPU and GPU for error-free GPU processing in the scenario where the GPU\n",
            "occasionally runs out-of-memory. The feature works like regular memory paging between CPU RAM\n",
            "and the disk. We use this feature to allocate paged memory for the optimizer states which are then\n",
            "automatically evicted to CPU RAM when the GPU runs out-of-memory and paged back into GPU\n",
            "memory when the memory is needed in the optimizer update step.\n",
            "QL ORA.Using the components described above, we define QLORAfor a single linear layer in\n",
            "the quantized base model with a single LoRA adapter as follows:\n",
            "YBF16=XBF16doubleDequant (cFP32\n",
            "1, ck-bit\n",
            "2,WNF4) +XBF16LBF16\n",
            "1LBF16\n",
            "2, (5)\n",
            "where doubleDequant (·)is defined as:\n",
            "doubleDequant (cFP32\n",
            "1, ck-bit\n",
            "2,Wk-bit) =dequant (dequant (cFP32\n",
            "1, ck-bit\n",
            "2),W4bit) =WBF16,(6)\n",
            "We use NF4 for Wand FP8 for c2. We use a blocksize of 64 for Wfor higher quantization precision\n",
            "and a blocksize of 256 for c2to conserve memory.\n",
            "For parameter updates only the gradient with respect to the error for the adapters weights∂E\n",
            "∂Liare\n",
            "needed, and not for 4-bit weights∂E\n",
            "∂W. However, the calculation of∂E\n",
            "∂Lientails the calculation of∂X\n",
            "∂W\n",
            "which proceeds via equation (5) with dequantization from storage WNF4to computation data type\n",
            "WBF16to calculate the derivative∂X\n",
            "∂Win BFloat16 precision.\n",
            "To summarize, QLORAhas one storage data type (usually 4-bit NormalFloat) and a computation\n",
            "data type (16-bit BrainFloat). We dequantize the storage data type to the computation data type\n",
            "to perform the forward and backward pass, but we only compute weight gradients for the LoRA\n",
            "parameters which use 16-bit BrainFloat.\n",
            "4 QLoRA vs. Standard Finetuning\n",
            "We have discussed how QLoRA works and how it can significantly reduce the required memory for\n",
            "finetuning models. The main question now is whether QLoRA can perform as well as full-model\n",
            "finetuning. Furthermore, we want to analyze the components of QLoRA including the impact of\n",
            "NormalFloat4 over standard Float4. The following sections will discuss the experiments that aimed\n",
            "at answering these questions.\n",
            "3https://docs.nvidia.com/cuda/cuda-c-programming-guide\n",
            "5\n",
            "\n",
            "Experimental setup. We consider three architectures (encoder, encoder-decoder, and decoder only)\n",
            "and compare QLoRA with 16-bit adapter-finetuning and with full-finetuning for models up to 3B. Our\n",
            "evaluations include GLUE [ 58] with RoBERTa-large [ 38], Super-NaturalInstructions (TKInstruct)\n",
            "[61] with T5 [ 49], and 5-shot MMLU [ 24] after finetuning LLaMA on Flan v2 [ 39] and Alpaca\n",
            "[55]. To additionally study the advantages of NF4 over other 4-bit data types, we use the setup of\n",
            "Dettmers and Zettlemoyer [13] and measure post-quantization zero-shot accuracy and perplexity\n",
            "across different models (OPT [ 72], LLaMA [ 57], BLOOM [ 52], Pythia [ 7]) for model sizes 125m -\n",
            "13B. We provide more details in the results section for each particular setup to make the results more\n",
            "readable. Full details in Appendix A.\n",
            "QLoRA-AllQLoRA-FFN\n",
            "QLoRA-AttentionAlpaca (ours)\n",
            "Stanford-Alpaca\n",
            "Model6061626364RougeL\n",
            "bits\n",
            "4\n",
            "16\n",
            "Figure 2: RougeL for LLaMA 7B models on the\n",
            "Alpaca dataset. Each point represents a run with a\n",
            "different random seed. We improve on the Stanford\n",
            "Alpaca fully finetuned default hyperparameters to\n",
            "construct a strong 16-bit baseline for comparisons.\n",
            "\u001b[0m\u001b[1;3;38;5;200mRetrieved node with id, entering: node-0\n",
            "\u001b[0m\u001b[1;3;34mRetrieving with query id node-0: Can you tell me about the Paged Optimizers?\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paged Optimizers are a feature used in QLORA to manage memory spikes during finetuning. This feature automatically pages memory between the CPU and GPU for error-free GPU processing in scenarios where the GPU occasionally runs out-of-memory. In QLORA, the optimizer states are allocated paged memory, which are then automatically evicted to CPU RAM when the GPU runs out-of-memory and paged back into GPU memory when the memory is needed in the optimizer update step. This feature helps to reduce the memory footprint of finetuning large language models, making it possible to finetune a quantized 4-bit model without any performance degradation.\n"
          ]
        }
      ],
      "source": [
        "response = query_engine_metadata.query(\n",
        "    \"Can you tell me about the Paged Optimizers?\"\n",
        ")\n",
        "print(str(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Storage\n",
        "\n",
        "Now, let's store your embeddings to Chroma DB to retrieve later."
      ],
      "metadata": {
        "id": "ztJoBxYV-w1i"
      },
      "id": "ztJoBxYV-w1i"
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize client, setting path to save data\n",
        "db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "\n",
        "# create collection\n",
        "chroma_collection = db.get_or_create_collection(\"QLoRa_knowledge_database\")\n",
        "\n",
        "# assign chroma as the vector_store to the context\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "# create your index\n",
        "vector_index_metadata_db = VectorStoreIndex(\n",
        "    all_nodes,\n",
        "    storage_context=storage_context,\n",
        "    service_context=service_context\n",
        ")"
      ],
      "metadata": {
        "id": "4D1YlEyw9oSI"
      },
      "id": "4D1YlEyw9oSI",
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compare the Outputs"
      ],
      "metadata": {
        "id": "B_4uHgO258mM"
      },
      "id": "B_4uHgO258mM"
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_retriver = \"Paged Optimizers are a feature used in QLoRA to allocate paged memory for optimizer states. This allows for automatic page-to-page transfers between CPU and GPU memory for error-free GPU processing when the GPU runs out of memory. This feature works like regular memory paging between CPU RAM and disk. By using this feature, QLoRA can significantly reduce the required memory for finetuning models. However, the paging process only occurs when processing mini-batches with long sequence lengths, which is rare. The default LoRA hyperparameters do not match 16-bit performance, and the most critical LoRA hyperparameter is the number of LoRA adapters used in total. LoRA on all linear transformer block layers is required to match full finetuning performance. NormalFloat data type significantly improves bit-for-bit accuracy gains compared to regular 4-bit Floats, and double quantization allows for a more fine-grained control over the memory footprint to fit models of certain size into certain GPUs.\"\n",
        "\n",
        "chunk_reference_retriver = \"Yes, according to the given context information, Paged Optimizers are a feature used in the QLoRA model to allocate paged memory for optimizer states. This feature automatically transfers memory between the CPU and GPU when the GPU runs out of memory, allowing for error-free GPU processing. This is done by using the NVIDIA unified memory feature, which works like regular memory paging between CPU RAM and the disk. This allows for more efficient use of memory during the optimization process.\"\n",
        "\n",
        "metadata_retriver = \"Paged Optimizers are a feature used in QLORA to manage memory spikes during finetuning. This feature automatically pages memory between the CPU and GPU for error-free GPU processing in scenarios where the GPU occasionally runs out-of-memory. In QLORA, the optimizer states are allocated paged memory, which are then automatically evicted to CPU RAM when the GPU runs out-of-memory and paged back into GPU memory when the memory is needed in the optimizer update step. This feature helps to reduce the memory footprint of finetuning large language models, making it possible to finetune a quantized 4-bit model without any performance degradation.\""
      ],
      "metadata": {
        "id": "9p6USoYB58ZG"
      },
      "id": "9p6USoYB58ZG",
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame()\n",
        "\n",
        "df.loc[0, 'baseline_retriver'] = baseline_retriver\n",
        "df.loc[0, 'chunk_reference_retriver'] = chunk_reference_retriver\n",
        "df.loc[0, 'metadata_retriver'] = metadata_retriver\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "gON9BsnU6zQx",
        "outputId": "2a0fe3a1-d89c-48f3-98a9-5135e0dd8fc2"
      },
      "id": "gON9BsnU6zQx",
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                   baseline_retriver  \\\n",
              "0  Paged Optimizers are a feature used in QLoRA t...   \n",
              "\n",
              "                            chunk_reference_retriver  \\\n",
              "0  Yes, according to the given context informatio...   \n",
              "\n",
              "                                   metadata_retriver  \n",
              "0  Paged Optimizers are a feature used in QLORA t...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b0ee9490-79a4-43e1-ba14-e7678f4d9c26\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>baseline_retriver</th>\n",
              "      <th>chunk_reference_retriver</th>\n",
              "      <th>metadata_retriver</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Paged Optimizers are a feature used in QLoRA t...</td>\n",
              "      <td>Yes, according to the given context informatio...</td>\n",
              "      <td>Paged Optimizers are a feature used in QLORA t...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b0ee9490-79a4-43e1-ba14-e7678f4d9c26')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b0ee9490-79a4-43e1-ba14-e7678f4d9c26 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b0ee9490-79a4-43e1-ba14-e7678f4d9c26');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_ee4282ec-83b5-467c-a14d-218ff00e6df9\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_ee4282ec-83b5-467c-a14d-218ff00e6df9 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# END"
      ],
      "metadata": {
        "id": "oXMc3Pvc5czk"
      },
      "id": "oXMc3Pvc5czk"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "59329bc8fa5846f6bf7b47472554704e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e69cd2a80e694ae386fda153ea41da87",
              "IPY_MODEL_c3714ca613384126a7dc2522c5565444",
              "IPY_MODEL_8deb050246744b698a9bcabadd15b203"
            ],
            "layout": "IPY_MODEL_bfc160d260144a848f26348d033ed6b7"
          }
        },
        "e69cd2a80e694ae386fda153ea41da87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5831f1486bf43f7860a26d05e2699c6",
            "placeholder": "​",
            "style": "IPY_MODEL_6718b533d6f24039b94fa17f4f8d8a66",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "c3714ca613384126a7dc2522c5565444": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7075de14925547c7987b937d824cb317",
            "max": 8,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_073f7e3ae9924b9c93ec4de08c3bbc3f",
            "value": 8
          }
        },
        "8deb050246744b698a9bcabadd15b203": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84fafb68d4ea4f15831ecc695474af9c",
            "placeholder": "​",
            "style": "IPY_MODEL_a8e2202bb7d74a87b83ee88f9b23b523",
            "value": " 8/8 [01:10&lt;00:00,  7.68s/it]"
          }
        },
        "bfc160d260144a848f26348d033ed6b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5831f1486bf43f7860a26d05e2699c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6718b533d6f24039b94fa17f4f8d8a66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7075de14925547c7987b937d824cb317": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "073f7e3ae9924b9c93ec4de08c3bbc3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "84fafb68d4ea4f15831ecc695474af9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8e2202bb7d74a87b83ee88f9b23b523": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}